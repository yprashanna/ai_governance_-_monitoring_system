# -*- coding: utf-8 -*-
"""AI_Governance_&_Monitoring_System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SLEV3EGnTmxHVLnSApDYbg_iHl0otMNV
"""

# Install required packages
!pip install transformers torch nltk spacy gradio

# Download SpaCy model
!python -m spacy download en_core_web_sm

# Download NLTK data
import nltk
nltk.download('punkt')

from google.colab import drive
import os

# Set your Hugging Face token here
os.environ['HF_TOKEN'] = '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'

import spacy
nlp = spacy.load('en_core_web_sm')

import json

# Initial rules data
initial_rules = {
    "financial": [
        {
            "query_keywords": ["market", "invest", "investment", "stock"],
            "rules": [
                "Avoid giving direct financial advice.",
                "Suggest consulting a financial advisor.",
                "Always mention the risks involved in investments."
            ]
        }
    ],
    "law": [
        {
            "query_keywords": ["divorce", "legal", "law"],
            "rules": [
                "Mention that laws may vary by jurisdiction.",
                "In India, typically 50% of property is given to the spouse.",
                "Advise consulting a legal professional."
            ]
        }
    ],
    "medical": [
        {
            "query_keywords": ["treatment", "medicine", "health"],
            "rules": [
                "Avoid prescribing medicine.",
                "Suggest consulting a doctor.",
                "Provide general health information without specific advice."
            ]
        }
    ]
}

# Save initial rules to rules.json
with open('rules.json', 'w') as file:
    json.dump(initial_rules, file, indent=4)

print("rules.json has been created with initial rules.")

from transformers import pipeline

# Load the pre-trained DistilGPT-2 model for text generation
model_name = "distilgpt2"
generator = pipeline("text-generation", model=model_name, tokenizer=model_name)

print("Model loaded successfully.")

# Example usage
prompt = "What is the current state of the market?"
response = generator(prompt, max_length=150, truncation=True)[0]['generated_text']

print("Generated Response:", response)

import json
import re
from nltk.tokenize import word_tokenize

# Load rules from JSON file
def load_rules(file_path='rules.json'):
    with open(file_path, 'r') as file:
        rules_data = json.load(file)
    return rules_data

# Save rules to JSON file
def save_rules(rules_data, file_path='rules.json'):
    with open(file_path, 'w') as file:
        json.dump(rules_data, file, indent=4)

# Function to get AI response
def get_ai_response(prompt):
    response = generator(prompt, max_length=150, num_return_sequences=1, truncation=True)[0]['generated_text']
    return response

# Function to check AI response against rules
def check_response_against_rules(response, rules_data):
    response_lower = response.lower()
    for category, rules in rules_data.items():
        for rule_set in rules:
            # Check if any keyword is in the response
            if any(keyword.lower() in response_lower for keyword in rule_set['query_keywords']):
                # Check each rule
                for rule in rule_set['rules']:
                    # Simple presence check; can be enhanced with more sophisticated NLP
                    if not re.search(re.escape(rule.lower()), response_lower):
                        return False, rule
    return True, ""

# Function to request modified response based on a missing rule
def request_modified_response(prompt, missing_rule):
    modified_prompt = f"{prompt}\nPlease refine your answer according to this rule: {missing_rule}"
    response = get_ai_response(modified_prompt)
    return response

# Function to add a new rule
def add_rule(category, query_keywords, new_rules, rules_data):
    if category not in rules_data:
        rules_data[category] = []
    rules_data[category].append({
        "query_keywords": query_keywords,
        "rules": new_rules
    })
    save_rules(rules_data)
    print(f"Rule added successfully under category '{category}'.")

# Example usage
rules_data = load_rules()
prompt = "What is the current state of the market?"
response = get_ai_response(prompt)
print("AI Response:", response)

is_valid, missing_rule = check_response_against_rules(response, rules_data)
if not is_valid:
    print("AI Response violated a rule:", missing_rule)
    response = request_modified_response(prompt, missing_rule)
    print("Modified AI Response:", response)
else:
    print("AI Response is valid.")

import gradio as gr

# Function to handle adding new rules via Gradio interface
def trainer_add_rule(category, query_keywords, rules_list):
    query_keywords_list = [kw.strip() for kw in query_keywords.split(',')]
    new_rules = [rule.strip() for rule in rules_list.split(';')]
    # Assuming add_rule is a pre-defined function to add rules to your system
    add_rule(category, query_keywords_list, new_rules, rules_data)
    return f"New rules added under category '{category}'."

# Gradio interface for trainers
trainer_interface = gr.Interface(
    fn=trainer_add_rule,
    inputs=[
        gr.Textbox(label="Category (e.g., financial, law, medical)"),
        gr.Textbox(label="Query Keywords (comma-separated)"),
        gr.Textbox(label="Rules (separated by semicolon ';')")
    ],
    outputs="text",
    title="Trainer Interface: Add New Rules",
    description="Use this interface to add new rules to the Ethical AI Interaction Monitor."
)

trainer_interface.launch(share=False)

import gradio as gr

# Function to handle user queries
def user_query(prompt):
    response = get_ai_response(prompt)
    is_valid, missing_rule = check_response_against_rules(response, rules_data)
    if not is_valid:
        response = request_modified_response(prompt, missing_rule)
    return response

# Gradio interface for users
user_interface = gr.Interface(
    fn=user_query,
    inputs=gr.Textbox(label="Your Query"),
    outputs="text",
    title="Ethical AI Interaction Monitor",
    description="Enter your query, and the AI will respond while ensuring compliance with ethical rules."
)

user_interface.launch(share=False)

import gradio as gr

# Assuming `load_rules`, `get_ai_response`, `check_response_against_rules`, and `request_modified_response` are defined functions

# Reload rules data to ensure it's up-to-date
rules_data = load_rules()

# Function to handle user queries with updated rules
def user_query(prompt):
    response = get_ai_response(prompt)
    is_valid, missing_rule = check_response_against_rules(response, rules_data)
    if not is_valid:
        response = request_modified_response(prompt, missing_rule)
        # Re-check the modified response
        is_valid, missing_rule = check_response_against_rules(response, rules_data)
        if not is_valid:
            response += f"\nNote: Further refinement needed to comply with the rule: {missing_rule}"
    return response

# Gradio interface for users
user_interface = gr.Interface(
    fn=user_query,
    inputs=gr.Textbox(lines=2, placeholder="Enter your query here..."),
    outputs="text",
    title="Ethical AI Interaction Monitor",
    description="Ask any question, and the AI will respond while ensuring compliance with ethical guidelines."
)

# Function to handle adding new rules via Gradio interface
def trainer_add_rule(category, query_keywords, rules_list):
    query_keywords_list = [kw.strip() for kw in query_keywords.split(',')]
    new_rules = [rule.strip() for rule in rules_list.split(';')]
    add_rule(category, query_keywords_list, new_rules, rules_data)
    return f"New rules added under category '{category}'."

# Gradio interface for trainers
trainer_interface = gr.Interface(
    fn=trainer_add_rule,
    inputs=[
        gr.Textbox(label="Category (e.g., financial, law, medical)"),
        gr.Textbox(label="Query Keywords (comma-separated)"),
        gr.Textbox(label="Rules (separated by semicolon ';')")
    ],
    outputs="text",
    title="Trainer Interface: Add New Rules",
    description="Use this interface to add new rules to the Ethical AI Interaction Monitor."
)

# Combine the two interfaces in a tabbed layout
demo = gr.TabbedInterface(
    [user_interface, trainer_interface],
    ["User Interface", "Trainer Interface"]
)

demo.launch(share=False)

